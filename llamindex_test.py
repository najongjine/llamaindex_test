import os
import sys
from dotenv import load_dotenv

# LlamaIndex ê´€ë ¨ ì„í¬íŠ¸
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.gemini import Gemini

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env íŒŒì¼ì—ì„œ GOOGLE_API_KEY ì½ê¸°)
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

if not google_api_key:
    print("âŒ ì—ëŸ¬: .env íŒŒì¼ì— GOOGLE_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

def init_rag_system():
    print("ğŸš€ [System] RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")

    # ---------------------------------------------------------
    # A. ì„ë² ë”© ëª¨ë¸ ì„¤ì • (encord-team/ebind-full)
    # ---------------------------------------------------------
    print("ğŸ“¥ [Embedding] ebind-full ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ ì˜¤ë˜ ê±¸ë¦¼)")
    
    # trust_remote_code=True : ì´ ëª¨ë¸ì€ ì»¤ìŠ¤í…€ ì½”ë“œê°€ ìˆì–´ì„œ ë°˜ë“œì‹œ ì¼œì•¼ í•¨
    embed_model = HuggingFaceEmbedding(
        model_name="encord-team/ebind-full",
        trust_remote_code=True,
        device="cpu"  # GPU ìˆìœ¼ë©´ "cuda"ë¡œ ë³€ê²½í•˜ì„¸ìš” (ì†ë„ ì°¨ì´ í¼)
    )

    # ---------------------------------------------------------
    # B. LLM ì„¤ì • (Google Gemini 1.5 Flash)
    # ---------------------------------------------------------
    print("ğŸ¤– [LLM] Google Gemini 1.5 Flash ì—°ê²° ì¤‘...")
    
    # ë‚˜ì¤‘ì— 2.5 ë‚˜ì˜¤ë©´ model_name="models/gemini-2.5-flash"ë¡œ ë°”ê¾¸ë©´ ë¨
    llm = Gemini(
        model="models/gemini-2.5-flash", 
        api_key=google_api_key,
        temperature=0.1 # 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ (RAGì— ìœ ë¦¬)
    )

    # ---------------------------------------------------------
    # C. ì „ì—­ ì„¤ì • (LlamaIndexì—ê²Œ "ì´ì œë¶€í„° ì´ê±° ì¨" ë¼ê³  ë“±ë¡)
    # ---------------------------------------------------------
    Settings.embed_model = embed_model
    Settings.llm = llm
    
    print("âœ… [System] ì„¤ì • ì™„ë£Œ!")

def main():
    # ì„¤ì • ì´ˆê¸°í™”
    init_rag_system()

    # 1. ë°ì´í„° ë¡œë“œ (./data í´ë”ì— ìˆëŠ” ëª¨ë“  íŒŒì¼ ì½ê¸°)
    if not os.path.exists("./data"):
        os.makedirs("./data")
        with open("./data/sample.txt", "w", encoding="utf-8") as f:
            f.write("LlamaIndexëŠ” ë°ì´í„° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ê°•ì‚¬ë‹˜ì€ ì„œë²„ ê°œë°œì ì¶œì‹ ì…ë‹ˆë‹¤.")
        print("âš ï¸ ./data í´ë”ê°€ ì—†ì–´ì„œ ìƒ˜í”Œ íŒŒì¼ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    print("ğŸ“š [Data] ë¬¸ì„œ ì¸ë±ì‹±(Vectorizing) ì‹œì‘...")
    documents = SimpleDirectoryReader("./data").load_data()
    
    # 2. ì¸ë±ìŠ¤ ìƒì„± (ì—¬ê¸°ì„œ ebind-fullì´ ì—´ì‹¬íˆ ë•ë‹ˆë‹¤)
    index = VectorStoreIndex.from_documents(documents)
    
    # 3. ì¿¼ë¦¬ ì—”ì§„ ìƒì„±
    query_engine = index.as_query_engine()

    # 4. ì§ˆë¬¸í•˜ê¸°
    print("\n" + "="*30)
    user_question = "íšŒì‚¬ì—ì„œ í‚¤ìš°ëŠ” ê°œ ì´ë¦„ì´ ë­ì„?"
    print(f"â“ ì§ˆë¬¸: {user_question}")
    
    response = query_engine.query(user_question)
    
    print(f"ğŸ’¡ ë‹µë³€: {response}")
    print("="*30)

if __name__ == "__main__":
    main()